<!DOCTYPE HTML>
<!--
	Escape Velocity by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Plans and Progress</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" href="images/favicon.ico">
	</head>
	<body class="no-sidebar is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<section id="header" class="wrapper">

					<!-- Logo -->
						<div id="logo">
							<h1><a href="index.html">Plans and Progress</a></h1>
							<p>development of S.K.I.M</p>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li class="current"><a href="mainpage.html">Home</a></li>
								<li>
									<a href="#">The Project&nbsp;</a>
									<ul>
										<li><a href="overview.html">Overview</a></li>
										<li>
											<a href="#">Detailed Description</a>
											<ul>
												<li><a href="aims.html">Aims</a></li>
												<li><a href="plansandprogress.html">Plans and Progress</a></li>
												<li><a href="scopeandlimits.html">Scope and Limits</a></li>
												<li><a href="toolsandtech.html">Tools and Technologies</a></li>
												<li><a href="testing.html">Testing</a></li>
												<li><a href="timeframe.html">Timeframe</a></li>
												<li><a href="risks.html">Risks</a></li>
											</ul>
										</li>
										<li><a href="skillsandjobs.html">Skills and Jobs</a></li>
									</ul>
								</li>
								<li><a href="team.html">The Team</a></li>
								<li><a href="reflection.html">Group Reflection</a></li>
							</ul>
						</nav>

				</section>

				<div id="intro" class="wrapper style1">
					<div class="title">Design</div>
					<div class="container">
						<p>The design team immediately started working on the design with the hardware team, and a mockup with SketchUp was within the first two months of the project’s commencement. Since the goal was to fit the two main components of the device, the Pixy 2 camera unit as well as the Raspberry Pi 4 as efficiently as possible, the device shell was made not too far off in size to the Raspberry Pi unit. The reason for this is that the Pixy 2 is much smaller (Pixy 2020) and can fit onto the Raspberry Pi (Raspberry Pi 2020). A simple case that was 3D was then created as an early prototype a month after the design was finished.</p>
							<div class="container">
								<div class="row aln-center">
									<div class="col-4 col-12-medium">
										<section class="highlight">
											<a href="#" class="image featured"><img src="images/hardware/Raspberry Pi.webp" alt="" /></a>
											<caption>Raspberry Pi Model B</caption>
										</section>
									</div>
									<div class="col-4 col-12-medium">
										<section class="highlight">
											<a href="#" class="image featured"><img src="images/hardware/pixy2.jpg" alt="" /></a>
											<caption>Pixy 2</caption>
										</section>
									</div>
									<div class="col-4 col-12-medium">
										<section class="highlight">
											<a href="#" class="image featured"><img src="images/hardware/Invisi_1.jpg" alt="" /></a>
											<caption>3D rendering of the final product</caption>
										</section>
									</div>
								</div>
							</div>
							<br>
							<p>This design was then deemed great both functionally and aesthetically. We then talked about the materials and potential colours with the hardware team and marketing team and agreed on them. Aluminium was chosen to encase the devices due to its lightness and durability, and most especially its inability to rust since the kitchen is a very humid setting. They are very easy to shape as well, and do not cost as much as other materials, which would help us create the best device at the best price. Aluminium casings are being tested and right now, we are working on creating a new design for a potential second generation device as well as new colours that could work.</p>
							<p><u>References</u></p>
							<ul style="list-style-position: inside;">
								<li>Pixy 2020, Introducing Pixy 2, Pixy, viewed 27 November 2020, https://pixycam.com/pixy2/</li>
								<li>Raspberry Pi 2020, Raspberry Pi 4 Tech Specs, Raspberry Pi, viewed 27 November 2020, https://www.raspberrypi.org/products/raspberry-pi-4-model-b/specifications/?resellerType=home</https:></li>
								<li>Almag 2020, Benefits of Aluminum, Almag, viewed 27 November 2020, https://almag.com/resources/benefits-of-aluminum/</li>
							</ul>
						</section>
					</div>
				</div>

			<!-- Main -->
				<div id="main" class="wrapper style2">
					<div class="title">Hardware</div>
					<div class="container">

						<!-- Content -->
							<div id="content">
								<article class="box post">
									<header class="style1">
										<h2>Hardware Used</h2>
									</header>
									<p>The project will use a raspberry pi as the base of operations. A raspberry pi is a small, low cost microcomputer that can be used for any number of applications. In our case, the raspberry pi will capture an image of your kitchen bench using an onboard camera, then send the image off to tensor flow where it will be analysed for a known product.</p>
									<p>We have attempted to make the project as slimline and as simple as possible. Both to reduce cost and the physical size of the device. Working with the dimensions of the raspberry pi, we have designed a 3d model in Google SketchUp that we will then 3D print to form the basis of the device.</p>
									<p>In this diagram you can see the general layout of the device from the bottom up as it will face downwards toward the counter. In this iteration we have the Raspberry Pi’s I/O accessible however with a consumer product these would be removed after installation of the required software. Atop the S.K.I.M are two LED indicators to provide feedback to the user on whether the product has found a match or not. These indicators also function as a button that will manually trigger a capture. The buttons are raised slightly to provide tactile feedback should the device be mounted in a spot that is hard to see e.g. under an overhead cupboard.</p>
									<div class="container">
										<div class="row aln-center">
											<div class="col-8 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img src="images/hardware/SKIMbyIceT.png" alt="" /></a>
												</section>
											</div>
										</div>
									</div>
									<p>This exploded view shows just how simple the product is in terms of hardware. The Raspberry Pi Camera is used for its simplicity and compatibility with the pi. With this model we have chosen to use the Raspberry Pi Model 4B, as it is the most powerful and up to date Raspberry Pi currently available. This is purely for testing purposes however as we are unsure as to just how much computing power the application will require. As machine learning can be quite intensive training a large dataset of images on the Pi would be a tricky procedure.</p>
									<div class="container">
										<div class="row aln-center">
											<div class="col-8 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img src="images/hardware/exploded.png" alt="" /></a>
												</section>
											</div>
										</div>
									</div>
									<br>
									<header class="style1">
										<h2>Training</h2>
									</header>
									<p>Machine learning and training is quite intensive and taxing on computer hardware and training a large data set on a Raspberry Pi would be quite time consuming. Therefore, we have decided to use a software solution called Lobe.Ai. Lobe can be executed on a windows pc making it very accessible, and easy to use. The training process is quite simple and once it is trained on a data set that training data can then be exported to a Tensor Flow file. This will allow us to train the Ai off device, alleviating much of the computing stress on the device.</p>
									<br>
									<header class="style1">
										<h2>The Pi</h2>
									</header>
									<div class="container">
										<div class="row aln-center">
											<div class="col-8 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img src="images/hardware/Raspberry Pi.webp" alt="" /></a>
												</section>
											</div>
										</div>
									</div>
									<p>Choosing the correct Raspberry Pi for this job could be a costly and time-consuming process, possibly requiring us to purchase numerous Pi devices to determine which is the most cost-effective solution. For this reason, we have decided that the first iteration the device will make use of a Raspberry Pi Model 4 B. This pathway may be the most expensive option upfront ($74.50 for a 2gb model and up to $130 for 8gb @ https://raspberry.piaustralia.com.au/products/raspberry-pi-4), but it provides us with the most hardware overhead should we run into optimisation issues where a lower model may not cut it. Further down the line in development, once we have truly discovered the processing needs of the software we may think about downgrading the hardware to reduce cost. This downgrade may even bring added benefits of a smaller footprint. The Pi 1 model A+ is currently only $42.90 and is significantly smaller than the 4B. This decrease in cost would have a dramatic impact on production capacity and would make the device accessible to more people.</p>
									<br>
									<header class="style1">
										<h2>The Camera</h2>
									</header>
									<div class="container">
										<div class="row aln-center">
											<div class="col-8 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img style="width: 22em; margin-left: 11em;"src="images/hardware/pixy2.jpg" alt="" /></a>
												</section>
											</div>
										</div>
									</div>
									<p>The choice for the camera was an easy one really. The Pixy 2 camera is much like the camera Raspberry Pi have on offer, only better in almost every imaginable way. The Pixy2 offers far better low light performance than the Pi Camera thanks to a bigger sensor, and can also be pushed to 60fps video which is great for grabbing a still out of for the live feed. The Pixy is connected to the Raspberry Pi via the camera header on the Pi, which means it also doesn’t take up any other I/O as it has its own dedicated connector. The object tracking algorithm that is included with the purchase of the Pixy 2 will also make tracking and identifying objects much easier.</p>
									<br>
									<header class="style1">
										<h2>The Case</h2>
									</header>
									<p>With the aid of CAD and 3D modelling, we were able to design an aesthetic and simple case for the device the would-be low cost and easy to manufacture. As seen in the cutaway the case fits very snuggly around the raspberry pi, and with future revisions could be made even smaller with a smaller raspberry pi as mentioned previously.</p>
									<p>Mounting the hardware inside the case is a fairly simple process thanks to tight tolerances. The Raspberry pi will mount on 4 mounting tabs via a friction fit and some strategic melting of said tabs. The camera is mounted to the case via a threaded lens hood, clamping the camera to the outer case. The buttons will be glued in place and have quick connectors on the GPIO pins of the Raspberry Pi to allow for assembly and disassembly as required. Mounting the device to a surface can be done in one of two methods. The first being with any of the 4 inbuilt slide on screw mounts. This provides a sturdy yet removable mounting process that will keep the device in place. The second method involves simply double-sided taping the device to the surface along the smooth faceplate of the device.</p>
									<div class="container">
										<div class="row aln-center">
											<div class="col-6 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img src="images/hardware/Invisi_1.jpg" alt="" /></a>
												</section>
											</div>
											<div class="col-6 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img src="images/hardware/Mounts.jpg" alt="" /></a>
												</section>
											</div>
										</div>
									</div>
									<p>The case would be initially 3D printed to allow for small run sizes and incremental changes without drastically effecting cost. Once the design has been refined to a stage where mass production is possible, the case would move to an injection mould process which would dramatically reduce cost and lead time; two key components to a successful product.</p>
									<br>
									<header class="style1">
										<h2>User Experience</h2>
									</header>
									<div class="container">
										<div class="row aln-center">
											<div class="col-8 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img src="images/hardware/TopSide.jpg" alt="" /></a>
												</section>
											</div>
										</div>
									</div>
									<p>The final hardware component to the SKIM is the illuminated button that sits next to the camera. The idea of this button is to provide visual feedback to the user about the operation of the device, and potentially debugging feedback for the development process. These two LED indicators will flash either Red or Green to let the user know if the device has successfully matched the item to on in the data base, or whether another attempt is needed. These lights also function as a push button for the user, should manual operation be preferred.</p>
									<br>
									<p><u>References</u></p>
									<ul>
										<li>Raspberry Pi 2020, Raspberry Pi Model 4B, Raspberry Pi Organisation, viewed 19 November 2020, https://www.raspberrypi.org/products/raspberry-pi-4-model-b</li>
										<li>PixyCam 2020, Pixy2 – PixyCam, Pixy, viewed 19 November 2020, https://pixycam.com/pixy2</li>
										<li>Microsoft 2020, Microsoft Lobe, Microsoft, viewed 19 November 2020, https://lobe.ai/</li>
										<li>TensorFlow 2020, Tensorflow Core | Machine Learning For Beginners and Experts, TensorFlow, viewed 19 November 2020, https://www.tensorflow.org/overview/</li>
									</ul>
								</article>
							</div>

					</div>
				</div>
				<div id="highlights" class="wrapper style3">
					<div class="title">Image Capture</div>
					<div class="container">
						<div class="row aln-center">
							<p>Throughout the past couple of months, the team and I have developed the idea of kitchen inventory manager and our plans for what each step of the process will involve. In this part of the report the discussion will revolve around the image capture process which is the initial system that needs to work successfully for the rest of the machine to perform.<br><br>The plan of how the image capture will operate is a constant discussion in meetings as we have realised there will have to be multiple modes of operation needed to reach a wide market of different needs. In the short term to get the product up and running the group has decided that the most suitable and feasible option is to have a manual button-activated image capture. As the product develops it is of consensus that we will need to move towards a more customer end-user friendly system. This includes using a motion sensor video feed that can send clear images to the TensorFlow learning system and be stored in the data base as a high-quality image.<br><br>We will also plan to integrate a voice technology system for the vision-impaired or for customers who would simply prefer this method. This plan will have to have separate learning system developed so would be planned for far later stage as the original system will be designed around a visual element.<br><br>This artefact has had changes to the original proposal presented due mostly to feasibility and reducing inaccuracies. The image capture design has been developed around a top-down view as it has proven to be the most suited. This mainly comes with element of security which is very important to the everyday customer. If the product does move towards a motion sensor based live video feed in the future, we do not want to have an open-ended camera in which the users are feeling like they are being recorded constantly.</p>
						</div>
					</div>
				</div>
				<div id="intro" class="wrapper style1">
					<div class="title">Machine Learning <br>and Visual Recognition</div>
					<div class="container">
						<p>Tensorflow was the starting point for the machine learning and visual recognition but to ensure we were choosing the right service we broadened our search a little to evaluate alternatives.</p>
						<p>Amazon Rekognition had a moderate set of recognition tools, many of which would not be utilised in the project. Pricing was deemed to be high at volume, with a basic cost of $1300 for the first million images processed per month (Amazon, n.d.). The marketing for this product felt aimed at big business, which was consistent with its cost.</p>
						<p>Microsoft Azure Cognitive Services Computer Vision service impressed us with the video analytics available, and was a possible contender with its impressive output detail and character recognition, as tested in the below image (Microsoft, n.d.) however the free account pricing was unclear, so this was parked for further exploration opportunity should Tensorflow Lite was not suitable.  </p>
						<div class="container">
							<div class="row aln-center">
								<div class="col-8 col-12-medium">
									<section class="highlight">
										<a href="#" class="image featured"><img src="images/Machine Learning and Visual Recognition/Microsoft-Azure-In-Action.PNG" alt="" /></a>
									</section>
								</div>
							</div>
						</div>
						<p>Google has several products aside from Tensorflow, including VisionAI which was the lead possible alternative from that suite. The service offers an API and while it might have been possible to retrain the service, it performed moderately when presented with a packaged item, however with three of the top five responses being wildly incorrect, the team had its doubts.</p>
						<div class="container">
							<div class="row aln-center">
								<div class="col-8 col-12-medium">
									<section class="highlight">
										<a href="#" class="image featured"><img src="images/Machine Learning and Visual Recognition/Google-VisionAI-In-Action.PNG" alt="" /></a>
									</section>
								</div>
							</div>
						</div>
						<p>Then after testing the service with multiple objects, this option was discounted.</p>
						<div class="container">
							<div class="row aln-center">
								<div class="col-8 col-12-medium">
									<section class="highlight">
										<a href="#" class="image featured"><img src="images/Machine Learning and Visual Recognition/Google-VisionAI-Fail.PNG" alt="" /></a>
									</section>
								</div>
							</div>
						</div>
						<p>IBM Watson was looked into but with no easy way to test without signing up for an account, and having viewed others’ tests of the service (Bertelli, 2017), we ranked this solution mid-pack.<br><br>We decided to proceed with Tensorflow Lite.</p>
						<p>For Visual Recognition to function, the service needs to “learn” what it is looking at, through being trained by humans. The original plan had been to train Tensorflow Lite directly by importing located and downloaded images, however during the course of researching, a member of the team discovered a previously unknown Microsoft product, Lobe, and suggested it as an alternative to Tensorflow. Three team members set about testing the viability of Lobe and compared results. While we loved the simple and graphical user interface of Lobe, and its intuitive nature, we found that it was limited to recognising one object at a time and on the surface did not appear to have an API or a way of delivering the resulting labels back to the Pi for transport to the project’s database.</p>
						<p>We agreed to use it test its capability in training the model, as importing images was relatively straight forward. A dataset of 101,000 food images was located (Tensorflow, 2020). The entire collection was queued within Lobe on a well-appointed Windows computer and instantly the CPU was stressed at 100% for several hours while trying to process the images.</p>
						<div class="container">
							<div class="row aln-center">
								<div class="col-8 col-12-medium">
									<section class="highlight">
										<a href="#" class="image featured"><img src="images/Machine Learning and Visual Recognition/Lobe-training-CPU-usage.JPG" alt="" /></a>
									</section>
								</div>
							</div>
						</div>
						<p>The first attempt ended after 33% of the images were processed, while the second attempt failed after 26% were processed. As the accuracy rate was 41%, we decided not to try this dataset a third time.</p>
						<div class="container">
							<div class="row aln-center">
								<div class="col-8 col-12-medium">
									<section class="highlight">
										<a href="#" class="image featured"><img src="images/Machine Learning and Visual Recognition/Lobe-training-41percent.JPG" alt="" /></a>
									</section>
								</div>
							</div>
						</div>
						<p>The third attempt was us taking ten images each of three food items and the model was tested against five images of each item. This yielded 100% accuracy, leading us to draw the conclusion that either the quality of the 101,000 images was too low, that there were too many components within the images causing the model to become confused, or that we were trying too much too soon.</p>
						<div class="container">
							<div class="row aln-center">
								<div class="col-6 col-12-medium">
									<section class="highlight">
										<a style="width: 35em;" href="#" class="image featured"><img src="images/Machine Learning and Visual Recognition/Test-image-sample.PNG" alt="" /></a>
									</section>
								</div>
								<div class="col-6 col-12-medium">
									<section class="highlight">
										<a style="width: 25em;" href="#" class="image featured"><img src="images/Machine Learning and Visual Recognition/Lobe-comparison.JPG" alt="" /></a>
									</section>
								</div>
							</div>
						</div>
						<p>The fourth attempt trained the model with a handful of objects on hand, with the model tested with video. This was remarkably accurate given the image was moving, there were occasionally more than one object in the frame and it was trained with a small dataset.</p>
						<a><iframe width="800" height="425" src="https://www.youtube.com/embed/rWuVc7_NKgI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></a>
						<p>The third model was chosen to upload due to its success rate. Further training would be needed to detect a wider range of items.</p>
						<br>
						<p><u>References</u></p>
						<ul style="list-style-position: inside;">
							<li>Bertelli A 2017, Visual Recognition APIs: IBM Watson vs Google Tensorflow, Above Intelligent AI/ML News Publication, viewed 19 November 2020, https://aboveintelligent.com/visual-recognition-apis-ibm-watson-vs-google-tensorflow-bca36ca1f1bd</li>
							<li>Amazon n.d., Amazon Rekognition pricing, Amazon, viewed 19 November 2020, https://aws.amazon.com/rekognition/pricing/?nc=sn&loc=4</li>
							<li>Google n.d., VisionAI, Google, viewed 19 November 2020, https://cloud.google.com/vision/</li>
							<li>Microsoft n.d., Microsoft Lobe, Microsoft, viewed 19 November 2020, https://lobe.ai/</li>
							<li>Microsoft (n.d.) Computer Vision, Microsoft, viewed 18 November 2020, https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/#features</li>
							<li>Tensorflow n.d., food101, Tensorflow, viewed 23 November, 2020  https://www.tensorflow.org/datasets/catalog/food101</li>
						</ul>
					</div>
				</div>
				<div id="main" class="wrapper style2">
					<div class="title">Database</div>
					<div class="container">

						<!-- Content -->
							<div id="content">
								<article class="box post">
									<p>The aim for the databases used in the S.K.I.M (Smart Kitchen Inventory Manager) project is to have multiple databases that can be addressed by other sub systems and to have the databases integrate with each other seamlessly.</p>
									<p>Multiple aspects of database technology need to be considered for this application. The software will need to be scalable, have compatibility with the operating system running on the hardware and with TensorFlow. It is preferable to use a database that is commonly used so that community support is widely available. Some options for database systems are SQLite and MongoDB (Choudhury, 2020).</p>
									<p>SQLite is an embedded Relational SQL database engine. It is fully self-contained and does not rely on separate server processes. The engine reads and writes to ordinary disk files that are cross platform and can be used by programs other than the database engine. This may be useful if the database is storing image files since SQLite will be able to interact with the files, and they will be available for separate processes to interact with them too. The engine library is compact, with all features the size can be less than 600 kilobytes. This would be beneficial to the primary function of the product since there would be more disk space for the files stored in the database. In terms of pricing, the SQLite database engine is very appealing because it is in the public domain. This means it does not require a license, and it is free for use. Companies can apply for a license for their own legal needs, but it is not required by the developers of the software. SQLite is open source but not open contribution, the team that developed it kept the quality of the engine at the forefront when writing the program, and it has resulted in it being one of the most reliable systems for storing information in a database (SQLite, 2020).</p>
									<p>Another database technology that is used in the machine learning space is MongoDB. It is heavily used for this purpose due to its flexibility and scalability. It is often used for web applications that incorporate machine learning into their backend. If the S.K.I.M project does scope creep to include more web app functionality this database management system may be useful. It also has the option to run locally on the Raspberry Pi in the Linux operating system (What Is MongoDB?, 2020), so if the project starts as a fully self-sufficient device and transitions into a product that incorporates cloud computing the MongoDB database can allow the changes to run smoothly. Since it is a distributed database that stores many files it may not be appropriate for this product, but it depends on the development vision of the team as to how the final product functions.</p>
									<p>At this stage of development, I suggest that SQLite is the option employed in the S.K.I.M product. Its small overhead on the system will benefit the other processing that needs to be done and the connectivity with other software’s will be an advantage. The software is regularly used on Raspberry Pi’s running Linux, so there is an abundance of community support.</p>
									<header class="style1">
										<h2>Lookup Database</h2>
									</header>
									<p>The look up database will be the larger database in use for the system. It will exist to provide data for several functions in the food classification stage.</p>
									<p>It will help to translate the output from TensorFlow into something more meaningful to the user. When the output is received by the lookup database processes, it will run the resulting image label through the database to see if there is a more commonly used name for that item. For example, the user may prefer to call spring onions by another name like green onions or vice versa, this database will hold user specific data to help classify the image with a label that they understand. The database could gather this information when it has that item scanned into it for the first time, from there it can save that user specific setting into the lookup database.</p>
									<p>This database can also be used to hold a list of previously held items, this serves to offer faster image recognition since the item has already been seen by the system. It may be possible to build up this “known items” database to the point where the system can recognise these items faster than unseen items.</p>
									<p>A problem that this database may encounter is that its size may grow to be too large for the onboard storage. During the testing phase this possibility would have to be explored to determine whether cloud storage and interconnectivity need to be introduced.</p>
									<header class="style1">
										<h2>Inventory Database</h2>
									</header>
									<p>The inventory database contains the data pertaining to the user’s inventory. It follows several processes, one of which is when the user scans an item into their inventory or removes an item from their inventory. Another being when the user views their inventory on the web application.</p>
									<p>The item addition and subtraction processes are depicted below. The diagram starts from the “Which button was pressed” query, and at that step, the image has already been processed by the TensorFlow classification model. It includes rules that account for if the removed item is not currently in the database, in this case the user is forced to begin the item scan again.</p>
									<p>The flow also interacts with the lookup database when it checks for alternate names in the “Does other name exist” query. Having both databases stored onboard, and in the same format will help to increase speed, improve compatibility between databases and ensure the continuity of the processes.</p>
									<div class="container">
										<div class="row aln-center">
											<div class="col-8 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img src="images/database/inventorydb1.png" alt="" /></a>
												</section>
											</div>
										</div>
									</div>
									<p>The interactivity between the inventory database and web application is shown in the diagram below. The process for the data output begins when the user initiates the application. The application makes requests to the webserver onboard the Raspberry Pi, which has access to the inventory database. The data is compiled in a manner that makes it compatible with the front-end interface of the application.</p>
									<div class="container">
										<div class="row aln-center">
											<div class="col-8 col-12-medium">
												<section class="highlight">
													<a href="#" class="image featured"><img src="images/database/inventorydb2.png" alt="" /></a>
												</section>
											</div>
										</div>
									</div>
									<p><u>References</u></p>
									<ul style="list-style-position: inside;">
										<li>Choudhury, A., 2020, Top Databases Used In Machine Learning Projects, Analytics India Magazine, viewed 21 November 2020, https://analyticsindiamag.com/top-databases-used-in-machine-learning-projects/</li>
										<li>Sqlite.org. 2020, About SQLite, viewed 23 November 2020, https://www.sqlite.org/about.html</li>
										<li>MongoDB. 2020, What Is MongoDB?, viewed 23 November 2020, https://www.mongodb.com/what-is-mongodb</li>
									</ul>
								</article>
							</div>

					</div>
				</div>
				<div id="highlights" class="wrapper style3">
					<div class="title">Marketing</div>
					<div class="container">
						<div class="row aln-center">
							<p>A marketing started creating a baseline plan for the product since the start of development. The main plan for marketing at that point was to create a large enough social media presence through posts and videos. Posts would mainly comprise of two main types of photos/media, namely attention-grabbing memes and informational pictures or videos. The first would be to catch a person’s attention during the first few moments they interact with the product or brand, and the informational picture would be to help keep the person interested by letting the potential customer know what the product is all about.<br><br>Attention-grabbing memes were planned and made during meetings with the marketing team, while informational pictures and videos were made throughout the entire development process of the product with a video created for the product in its three stages, while it was a 3D mock-up, a 3D printed model and as it was the final product to be sold to consumers. Videos made in collaboration with technology based Youtube channels are scheduled to be filmed two weeks before the products official release date.<br><br>In order to properly utilize all the content that will be put out, three main channels of social media were created namely Facebook, Instagram and Twitter. These social media outlets target different types of people and this can be categorized into the demographics and is the reason why all three are crucial is the marketing plan.<br><br>Facebook has a primarily adult userbase with majority of the users being middle-aged adults. This would make it great to advertise the product on this medium since it is much more common for adults to bear the responsibility of keeping track of the food at home. Moreover, Facebook is a great platform for building up a brand or a business due to users logging on much more often than other platforms, and because of the availability of the data that shows the engagement on your page made available through the site. It also utilizes both pictures/videos and words, making it a flexible and overall encompassing marketing platform.<br><br>Twitter on the other hand has userbase that is mainly made of younger people. Despite what the demographics may suggest, it is still a great platform to advertise a product targeted at adults since messages written on Twitter spread very quickly. This would be a great platform to place the attention-grabbing memes or pictures to reach a large audience, and the youth may also inform their parents or older peers about what they had seen. Journalists may also pick up on tweets that we send since more opinionated or informative messages could be shared on this platform to create interest.<br><br>Lastly, Instagram is a sort of happy medium in between Facebook and Twitter in terms of demographics. Young adults are the main users of Instagram, and the website focuses on exploring different aesthetics. This would be a great platform to build our brand identity through photos as well as catch the attention of users who don not typically use the first two platforms. (Fouche, K., 2019)</p>
							<br>
							<p><u>References</u></p>
							<ul style="list-style-position: inside;">
								<li>Fouche, K., 2019, Twitter Vs Facebook Vs Instagram Vs LinkedIn: Which is Right for You?, viewed 27 November 2020, https://blog.pixelfish.com.au/twitter-vs-facebook-vs-instagram-vs-linkedin</li>
							</ul>
						</div>
					</div>
				</div>
		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>